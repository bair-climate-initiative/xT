# xView3-detection

## Install 

First install the environment, which can be done as follows for CUDA 11.6. 

```bash
conda env create -f environment.yml
conda run -n scale python -m pip install --no-cache-dir -r requirements.txt
# Clean up
sync && conda clean --all --yes && sync 
rm -rf ~/.cache/pip/*
```

## Single Training Runs on Xview Dataset

Now training is taken care of by Hydra. Simply run 
```
torchrun --nproc_per_node=8 --master_port 47769 train_val_segmentor.py
```
and the default config will run. If you need to change some config settings, such as the launch paths, you can provide it in the command line as something like 
```
torchrun --nproc_per_node=8 --master_port 47769 train_val_segmentor.py data=xview3_cluster
```
where ```xview3_cluster``` is a YAML file in ```configs/data/``` specifying that specific config.

You can also override individual values in the command line, e.g. 
```
torchrun --nproc_per_node=8 --master_port 47769 train_val_segmentor.py data=xview3_cluster data.dir=/shared/ritwik/data/xview3
```

Notice the defaults are overriden by the configs in ```base_config.yaml```. The list of settings there is for your convenience to see at a glance what options are being provided in one place. 

## Old Instructions
### Single Training Runs on Xview Dataset

Generally training is interfaced through the ```train_xview.sh``` script.

```
./train_xview.sh  <num gpu> /shared/ritwik/data/xview3 /shared/ritwik/data/xview3/shoreline/validation <output dir> 77 <config name: e.g v2l> <port> <wandb run title> <input_scale> <optional --resume ckpt name>
```

Note that 77 is the val fold name.

There are example scripts in the ```run/``` folder. 

### Inference Xview Dataset

This only works for model checkpoint generated by local training, it does *NOT* apply to downloaded weights

```
./inference_xview.sh  <num gpu>  /shared/ritwik/data/xview3 /shared/ritwik/data/xview3/shoreline/public <output dir> 9999 <config> <port> <wandb run title> <input_scale> <optional --resume ckpt name>
```


## Large Scale Runs on Cluster

## Run jobs locally

e.g.
```
./cluster/main_exp_local.sh  ~/xView3_second_place/cluster/exps/512/swin_t_bs_4_p0.5.sh
```

where the first argument is the actual config file

## Run jobs on cluster

e.g.
```
/cluster/main_exp_cluster.sh  ~/xView3_detection/cluster/exps/512/swin_t_bs_4_p0.5.sh
```

where the first argument is the actual config file

## Run everything in folder
```
./cluster/main_exp_all.sh ~/xView3_second_place/cluster/main_exp_local.sh
``` 
or 

```
./cluster/main_exp_all.sh ~/xView3_detection/cluster/main_exp_cluster.sh
```
This will loop over all files in exps folder

# Compile files on runtime

Run `make build`


